{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10786172,"sourceType":"datasetVersion","datasetId":6693476}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and installs","metadata":{}},{"cell_type":"code","source":"!pip install pypots pygrinder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:45:55.721475Z","iopub.execute_input":"2025-02-21T12:45:55.721844Z","iopub.status.idle":"2025-02-21T12:46:01.806026Z","shell.execute_reply.started":"2025-02-21T12:45:55.721817Z","shell.execute_reply":"2025-02-21T12:46:01.805122Z"}},"outputs":[{"name":"stdout","text":"Collecting pypots\n  Downloading pypots-0.9-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pygrinder\n  Downloading pygrinder-0.7-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from pypots) (3.12.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pypots) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pypots) (1.13.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from pypots) (1.13.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from pypots) (0.8.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pypots) (2.2.3)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from pypots) (0.12.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pypots) (3.7.5)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from pypots) (2.17.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pypots) (1.2.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pypots) (2.5.1+cu121)\nCollecting tsdb>=0.6.1 (from pypots)\n  Downloading tsdb-0.6.2-py3-none-any.whl.metadata (13 kB)\nCollecting benchpots>=0.3.2 (from pypots)\n  Downloading benchpots-0.3.2-py3-none-any.whl.metadata (9.5 kB)\nCollecting ai4ts (from pypots)\n  Downloading ai4ts-0.0.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pypots) (2024.12.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->pypots) (1.3.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tsdb>=0.6.1->pypots) (4.67.1)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tsdb>=0.6.1->pypots) (19.0.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tsdb>=0.6.1->pypots) (2.32.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->pypots) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->pypots) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->pypots) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->pypots) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->pypots) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->pypots) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pypots) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pypots) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->pypots) (2025.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pypots) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pypots) (3.5.0)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (1.68.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->pypots) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->pypots) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pypots) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pypots) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->pypots) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->pypots) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tsdb>=0.6.1->pypots) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tsdb>=0.6.1->pypots) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tsdb>=0.6.1->pypots) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tsdb>=0.6.1->pypots) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->pypots) (2024.2.0)\nDownloading pypots-0.9-py3-none-any.whl (539 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m539.8/539.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pygrinder-0.7-py3-none-any.whl (24 kB)\nDownloading benchpots-0.3.2-py3-none-any.whl (29 kB)\nDownloading tsdb-0.6.2-py3-none-any.whl (32 kB)\nDownloading ai4ts-0.0.3-py3-none-any.whl (13 kB)\nInstalling collected packages: ai4ts, tsdb, pygrinder, benchpots, pypots\nSuccessfully installed ai4ts-0.0.3 benchpots-0.3.2 pygrinder-0.7 pypots-0.9 tsdb-0.6.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pygrinder import mcar, mnar_t, calc_missing_rate\nfrom typing import Dict, Tuple\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom math import sqrt\nimport random\n\n# Set global seed\nSEED = 23\nnp.random.seed(SEED)\nrandom.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:46:01.807460Z","iopub.execute_input":"2025-02-21T12:46:01.807805Z","iopub.status.idle":"2025-02-21T12:46:05.944035Z","shell.execute_reply.started":"2025-02-21T12:46:01.807775Z","shell.execute_reply":"2025-02-21T12:46:05.943251Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Amputation - PyGrinder","metadata":{}},{"cell_type":"code","source":"class PyGrinderExec:\n    def __init__(self, X: np.ndarray):\n        \"\"\"\n        Initialize with original 2D data array\n        \n        Parameters:\n        X: Original data with shape (timesteps, features)\n        \"\"\"\n        self.original_data = X.copy()\n        self.ground_truth_mcar = None\n        self.ground_truth_mnar = None\n        self.missing_mask_mcar = None\n        self.missing_mask_mnar = None\n        \n    def generate_missing(self, \n                        mcar_prob: float = 0.1,\n                        mnar_cycle: float = 20,\n                        mnar_pos: float = 10,\n                        mnar_scale: float = 0.1) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Generate missing values using both MCAR and MNAR-t methods\n        \n        Parameters:\n        mcar_prob: Probability of missing values for MCAR\n        mnar_cycle: Cycle parameter for MNAR-t\n        mnar_pos: Position parameter for MNAR-t\n        mnar_scale: Scale parameter for MNAR-t\n        \n        Returns:\n        Dict containing corrupted datasets and ground truth\n        \"\"\"\n        # Reshape to 3D for PyGrinder\n        X_3d = self.original_data[np.newaxis, :, :]\n        \n        # Generate MCAR missing values\n        X_mcar = mcar(X_3d, p=mcar_prob)[0]  # Get back to 2D\n        self.missing_mask_mcar = np.isnan(X_mcar)\n        self.ground_truth_mcar = self.original_data[self.missing_mask_mcar]\n        \n        # Generate MNAR-t missing values\n        X_mnar = mnar_t(X_3d, cycle=mnar_cycle, pos=mnar_pos, scale=mnar_scale)[0]  # Get back to 2D\n        self.missing_mask_mnar = np.isnan(X_mnar)\n        self.ground_truth_mnar = self.original_data[self.missing_mask_mnar]\n        \n        return {\n            'X_mcar': X_mcar,\n            'X_mnar': X_mnar,\n            'ground_truth_mcar': self.ground_truth_mcar,\n            'ground_truth_mnar': self.ground_truth_mnar,\n            'missing_mask_mcar': self.missing_mask_mcar,\n            'missing_mask_mnar': self.missing_mask_mnar\n        }\n    \n    def evaluate_imputation(self, \n                          X_imputed_mcar: np.ndarray, \n                          X_imputed_mnar: np.ndarray) -> Dict[str, Dict[str, float]]:\n        \"\"\"\n        Evaluate imputation results against ground truth\n        \n        Parameters:\n        X_imputed_mcar: Imputed data for MCAR case\n        X_imputed_mnar: Imputed data for MNAR case\n        \n        Returns:\n        Dictionary with error metrics for both methods\n        \"\"\"\n        if self.ground_truth_mcar is None or self.ground_truth_mnar is None:\n            raise ValueError(\"Must call generate_missing() first\")\n            \n        def calculate_metrics(pred: np.ndarray, true: np.ndarray, mask: np.ndarray) -> Dict[str, float]:\n            pred_missing = pred[mask]\n            mse = np.mean((pred_missing - true) ** 2)\n            mae = np.mean(np.abs(pred_missing - true))\n            rmse = np.sqrt(mse)\n            return {\n                #'MSE': mse,\n                'MAE': mae,\n                'RMSE': rmse\n            }\n        \n        return {\n            'MCAR': calculate_metrics(X_imputed_mcar, self.ground_truth_mcar, self.missing_mask_mcar),\n            'MNAR': calculate_metrics(X_imputed_mnar, self.ground_truth_mnar, self.missing_mask_mnar)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:46:05.945352Z","iopub.execute_input":"2025-02-21T12:46:05.945854Z","iopub.status.idle":"2025-02-21T12:46:05.954283Z","shell.execute_reply.started":"2025-02-21T12:46:05.945828Z","shell.execute_reply":"2025-02-21T12:46:05.953401Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"SAMPLE = True\nRESAMPLE = False\nRESAMPLING = '10min'\nTARGET = \"p (mbar)\"\n\nif SAMPLE:\n    n_rows = 100000\nelse:\n    n_rows = None\n\nMCAR_PROB=0.2\nMNAR_SCALE=0.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:46:45.547156Z","iopub.execute_input":"2025-02-21T12:46:45.547640Z","iopub.status.idle":"2025-02-21T12:46:45.552548Z","shell.execute_reply.started":"2025-02-21T12:46:45.547599Z","shell.execute_reply":"2025-02-21T12:46:45.551537Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def resample_climate(original_df, every='1h'):\n\n    resampled_df = (original_df\n                     .group_by_dynamic(\"Date Time\", every=every)\n                     .agg(pl.all()\n                          .mean()))\n\n    return resampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:46:45.737607Z","iopub.execute_input":"2025-02-21T12:46:45.737985Z","iopub.status.idle":"2025-02-21T12:46:45.742357Z","shell.execute_reply.started":"2025-02-21T12:46:45.737939Z","shell.execute_reply":"2025-02-21T12:46:45.741371Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data = pl.read_parquet(\"/kaggle/input/climate-ts-imputation/climate.parquet\", \n                        n_rows=n_rows)\n\nif RESAMPLE:\n    data = resample_climate(data, every=RESAMPLING)\n    \n# Assuming your data has a 'Date Time' column.\nprint(\"\\nFull dataset date range:\")\nprint(f\"  Start: {data['Date Time'].min()}\")\nprint(f\"  End:   {data['Date Time'].max()}\")\n\nprint(\"Data size for imputation and train: \", data.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:46:45.896857Z","iopub.execute_input":"2025-02-21T12:46:45.897156Z","iopub.status.idle":"2025-02-21T12:46:45.928164Z","shell.execute_reply.started":"2025-02-21T12:46:45.897134Z","shell.execute_reply":"2025-02-21T12:46:45.927341Z"}},"outputs":[{"name":"stdout","text":"\nFull dataset date range:\n  Start: 2009-01-01 00:10:00\n  End:   2010-11-25 11:00:00\nData size for imputation and train:  (100000, 15)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Testing PyGrinder in 1 year subset and visualize","metadata":{}},{"cell_type":"code","source":"if not SAMPLE:\n    y1_data = data.filter(pl.col(\"Date Time\").dt.year() == 2014)\nelse:\n    y1_data = data.clone()\n\n# Convert to numpy\ny_y1 = y1_data.drop(\"Date Time\").select(TARGET).to_numpy() # Save T to forecast\nX_y1 = y1_data.drop([\"Date Time\", TARGET]).to_numpy()\n\n# Scaling --> Difference between \"mean\" and \"others\" increase a lot.\nfrom sklearn.preprocessing import StandardScaler\nscalerX_y1 = StandardScaler().fit(X_y1)\nscalery_y1 = StandardScaler().fit(y_y1)\n\nX_y1 = scalerX_y1.transform(X_y1)\ny_y1 = scalery_y1.transform(y_y1)\n\n# Initialize with your 2D data\nexecutor = PyGrinderExec(X_y1)\n\n# Generate missing values (returns both MCAR and MNAR-t corrupted data and ground truth)\nresults = executor.generate_missing(\n    mcar_prob=MCAR_PROB,           # 10% missing for MCAR\n    mnar_cycle=20,           # MNAR-t parameters\n    mnar_pos=10,\n    mnar_scale=MNAR_SCALE\n)\n\nmcar_rate = calc_missing_rate(results['X_mcar'])\nmnar_rate = calc_missing_rate(results['X_mnar'])\nprint(f\"MCAR missing rate: {100*mcar_rate:.2f}%\")\nprint(f\"MNAR missing rate: {100*mnar_rate:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:46:46.164881Z","iopub.execute_input":"2025-02-21T12:46:46.165175Z","iopub.status.idle":"2025-02-21T12:46:46.403289Z","shell.execute_reply.started":"2025-02-21T12:46:46.165150Z","shell.execute_reply":"2025-02-21T12:46:46.402243Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"MCAR missing rate: 19.95%\nMNAR missing rate: 20.18%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def plot_missing_pattern(X: np.ndarray, \n                        title: str = \"Missing Values Pattern\",\n                        figsize: tuple = (8, 5)):\n    \"\"\"\n    Visualize missing values pattern in the dataset\n    \n    Parameters:\n    X: Input array with missing values (NaN)\n    title: Plot title\n    figsize: Figure size (width, height)\n    \"\"\"\n    # Create missing values mask (True where value is missing)\n    missing_mask = np.isnan(X)\n    \n    # Calculate missing percentage\n    missing_pct = np.mean(missing_mask) * 100\n    \n    # Create figure\n    plt.figure(figsize=figsize)\n    \n    # Create heatmap\n    sns.heatmap(missing_mask, \n                cbar=False,\n                cmap='binary',\n                yticklabels=False)\n    \n    # Customize plot\n    plt.title(f\"{title}\\nTotal Missing: {missing_pct:.1f}%\")\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Date Time\")\n    \n    # Show plot\n    plt.tight_layout()\n    plt.savefig(f\"missing_pattern_{RESAMPLING}.png\")\n    plt.savefig(f\"missing_pattern_{RESAMPLING}.svg\")\n    plt.show()\n\n# For MCAR corrupted data\nplot_missing_pattern(\n    results['X_mcar'],\n    title=\"MCAR Missing Pattern\",\n)\n\n# For MNAR corrupted data\nplot_missing_pattern(\n    results['X_mnar'],\n    title=\"MNAR Missing Pattern\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:29.466633Z","iopub.execute_input":"2025-02-21T10:47:29.466868Z","iopub.status.idle":"2025-02-21T10:47:33.584010Z","shell.execute_reply.started":"2025-02-21T10:47:29.466826Z","shell.execute_reply":"2025-02-21T10:47:33.583120Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Grinder whole data","metadata":{}},{"cell_type":"code","source":"# Convert to numpy\ny = data.drop(\"Date Time\").select(TARGET).to_numpy() # Save T to forecast\nX = data.drop([\"Date Time\", TARGET]).to_numpy()\n\n# Scaling --> Difference between \"mean\" and \"others\" increase a lot.\nfrom sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X)\nscalery = StandardScaler().fit(y)\n\nX = scalerX.transform(X)\ny = scalery.transform(y)\n\n# Initialize with 2D data\nexecutor = PyGrinderExec(X)\n\n# Generate missing values (returns both MCAR and MNAR-t corrupted data and ground truth)\nresults = executor.generate_missing(\n    mcar_prob=MCAR_PROB,           # 20% missing for MCAR\n    mnar_scale=MNAR_SCALE\n)\n\n# Access the corrupted datasets and ground truth\nX_mcar = results['X_mcar']\nX_mnar = results['X_mnar']\nground_truth_mcar = results['ground_truth_mcar']\nground_truth_mnar = results['ground_truth_mnar']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:33.587311Z","iopub.execute_input":"2025-02-21T10:47:33.587533Z","iopub.status.idle":"2025-02-21T10:47:33.607134Z","shell.execute_reply.started":"2025-02-21T10:47:33.587514Z","shell.execute_reply":"2025-02-21T10:47:33.606278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imputation","metadata":{}},{"cell_type":"markdown","source":"## Perform \"simple\" imputations (mean, interpolation, KNN)","metadata":{}},{"cell_type":"code","source":"def linear_interpolate(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs linear interpolation on each column of X to fill missing values.\n    \n    Parameters\n    ----------\n    X : np.ndarray of shape (n_samples, n_features)\n        Input data with missing values (NaN) you'd like to fill.\n\n    Returns\n    -------\n    np.ndarray\n        A copy of X with all missing values filled by linear interpolation.\n    \"\"\"\n    # Convert to DataFrame for easy interpolation\n    df = pd.DataFrame(X)\n    \n    # Apply linear interpolation along rows (axis=0)\n    # limit_direction='both' handles NaNs at the start/end\n    df_interpolated = df.interpolate(\n        method=\"linear\", \n        limit_direction=\"both\", \n        axis=0\n    )\n    \n    # If any NaNs remain (e.g., entire column was NaN), fill forward/backward\n    df_interpolated = df_interpolated.ffill().bfill()\n    \n    # Return as numpy array\n    return df_interpolated.values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:33.608411Z","iopub.execute_input":"2025-02-21T10:47:33.608613Z","iopub.status.idle":"2025-02-21T10:47:33.612509Z","shell.execute_reply.started":"2025-02-21T10:47:33.608596Z","shell.execute_reply":"2025-02-21T10:47:33.611441Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_imputations(X_mcar: np.ndarray, X_mnar: np.ndarray, n_neighbors: int = 5):\n    \"\"\"\n    Perform three imputation strategies: Simple (Mean), Interpolation, and KNN.\n    \n    Parameters\n    ----------\n    X_mcar : np.ndarray\n        MCAR dataset with missing values\n    X_mnar : np.ndarray\n        MNAR dataset with missing values\n    n_neighbors : int, default=5\n        Number of neighbors for KNNImputer\n    \n    Returns\n    -------\n    dict\n        Dictionary of imputed results for MCAR and MNAR with keys:\n            'mean_mcar', 'mean_mnar'\n            'interp_mcar', 'interp_mnar'\n            'knn_mcar', 'knn_mnar'\n    \"\"\"\n    # Initialize mean & KNN imputers\n    mean_imputer = SimpleImputer(strategy='mean')\n    knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n    \n    # 1. Mean Imputation\n    X_imputed_mcar_mean = mean_imputer.fit_transform(X_mcar)\n    X_imputed_mnar_mean = mean_imputer.fit_transform(X_mnar)\n    \n    # 2. Interpolation Imputation \n    X_imputed_mcar_inter = linear_interpolate(X_mcar)\n    X_imputed_mnar_inter = linear_interpolate(X_mnar)\n    \n    # 3. KNN Imputation\n    X_imputed_mcar_knn = knn_imputer.fit_transform(X_mcar)\n    X_imputed_mnar_knn = knn_imputer.fit_transform(X_mnar)\n    \n    return {\n        'mean_mcar': X_imputed_mcar_mean,\n        'mean_mnar': X_imputed_mnar_mean,\n        'interp_mcar': X_imputed_mcar_inter,\n        'interp_mnar': X_imputed_mnar_inter,\n        'knn_mcar': X_imputed_mcar_knn,\n        'knn_mnar': X_imputed_mnar_knn\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:33.613252Z","iopub.execute_input":"2025-02-21T10:47:33.613462Z","iopub.status.idle":"2025-02-21T10:47:33.618487Z","shell.execute_reply.started":"2025-02-21T10:47:33.613443Z","shell.execute_reply":"2025-02-21T10:47:33.617784Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform \"simple\" imputations\nimputed_results = perform_imputations(\n    results['X_mcar'], \n    results['X_mnar'],\n    n_neighbors=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:33.619116Z","iopub.execute_input":"2025-02-21T10:47:33.619302Z","iopub.status.idle":"2025-02-21T10:47:50.776185Z","shell.execute_reply.started":"2025-02-21T10:47:33.619285Z","shell.execute_reply":"2025-02-21T10:47:50.775395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputed_results.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:48:18.511760Z","iopub.execute_input":"2025-02-21T10:48:18.512076Z","iopub.status.idle":"2025-02-21T10:48:18.516584Z","shell.execute_reply.started":"2025-02-21T10:48:18.512023Z","shell.execute_reply":"2025-02-21T10:48:18.515666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SAITS Imputation\n\nTo use **SAITS** on my single time series:\n\n- I reshaped my single time series into multiple windows of shape `(N, window_size, features)` so SAITS could see multiple “mini-sequences” instead of one long array.  \n- By doing this, the transformer-based model can better learn temporal dependencies and cross-feature relationships.  \n- For a fast implementation and quick demo, I skipped the usual best practices—like creating a validation set and separately imputing the test set—and instead demonstrated SAITS directly on my training data.  \n- After SAITS completed the imputation window-by-window, I averaged any overlapping predictions to restore the final shape.","metadata":{}},{"cell_type":"code","source":"from pypots.imputation import SAITS\nfrom pypots.utils.metrics import calc_mae","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:58:33.839028Z","iopub.execute_input":"2025-02-21T10:58:33.839390Z","iopub.status.idle":"2025-02-21T10:58:33.842831Z","shell.execute_reply.started":"2025-02-21T10:58:33.839364Z","shell.execute_reply":"2025-02-21T10:58:33.842126Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reshape corrupted data for SAITS","metadata":{}},{"cell_type":"code","source":"def create_windows(\n    X_2d: np.ndarray, \n    window_size: int, \n    step: int = 1\n) -> np.ndarray:\n    \"\"\"\n    Converts a 2D time-series array (T, F) into overlapping windows (N, window_size, F).\n\n    Parameters:\n    - X_2d (np.ndarray): The input 2D array with shape (T, F), where:\n        * T: Number of time steps\n        * F: Number of features\n    - window_size (int): The size of each window (number of time steps per window).\n    - step (int): The stride or step size for moving the window. Default is 1.\n    \"\"\"\n    T, F = X_2d.shape\n    windows = []\n    for start in range(0, T - window_size + 1, step):\n        end = start + window_size\n        windows.append(X_2d[start:end, :])\n    return np.array(windows)  # (N, window_size, F)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:58:34.059169Z","iopub.execute_input":"2025-02-21T10:58:34.059441Z","iopub.status.idle":"2025-02-21T10:58:34.063671Z","shell.execute_reply.started":"2025-02-21T10:58:34.059419Z","shell.execute_reply":"2025-02-21T10:58:34.062849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WINDOW = 28 #(una semana)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:59:50.033172Z","iopub.execute_input":"2025-02-21T10:59:50.033526Z","iopub.status.idle":"2025-02-21T10:59:50.036961Z","shell.execute_reply.started":"2025-02-21T10:59:50.033491Z","shell.execute_reply":"2025-02-21T10:59:50.036137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train and impute","metadata":{}},{"cell_type":"code","source":"def impute_and_evaluate(X_corrupted, X_original, window_size=28, epochs=10):\n    \"\"\"Imputes missing values in a corrupted dataset using SAITS and evaluates MAE.\"\"\"\n\n    # Create sliding windows for the corrupted and original datasets\n    X_corrupted_3d = create_windows(X_corrupted, window_size=window_size, step=1)\n    X_original_3d = create_windows(X_original, window_size=window_size, step=1)\n\n    # Prepare dataset for SAITS\n    dataset = {\"X\": X_corrupted_3d}\n\n    # Initialize SAITS model\n    saits = SAITS(\n        n_steps=window_size,\n        n_features=X_corrupted_3d.shape[2],\n        n_layers=2,\n        d_model=256,\n        d_ffn=128,\n        n_heads=4,\n        d_k=64,\n        d_v=64,\n        dropout=0.1,\n        epochs=epochs,\n        diagonal_attention_mask=True,\n    )\n\n    # Train SAITS using the corrupted dataset\n    saits.fit(dataset)\n\n    # Perform imputation\n    imputed_X = saits.impute(dataset)\n\n    # Create a mask for artificially missing values (where X_corrupted_3d is NaN but X_original_3d is not)\n    missing_mask_3d = np.isnan(X_corrupted_3d) & ~np.isnan(X_original_3d)\n\n    # Calculate MAE using the mask\n    mae = calc_mae(\n        imputed_X,\n        np.nan_to_num(X_original_3d),  # Replace NaNs with 0 (or another default value)\n        missing_mask_3d\n    )\n\n    print(f\"MAE (MCAR, {window_size} steps): {mae:.4f}\")\n\n    return imputed_X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:58:39.065192Z","iopub.execute_input":"2025-02-21T10:58:39.065481Z","iopub.status.idle":"2025-02-21T10:58:39.071148Z","shell.execute_reply.started":"2025-02-21T10:58:39.065460Z","shell.execute_reply":"2025-02-21T10:58:39.070265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"saits_imputed_mcar = impute_and_evaluate(X_mcar, X, window_size=WINDOW, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:00:04.570402Z","iopub.execute_input":"2025-02-21T11:00:04.570719Z","iopub.status.idle":"2025-02-21T11:01:04.291840Z","shell.execute_reply.started":"2025-02-21T11:00:04.570691Z","shell.execute_reply":"2025-02-21T11:01:04.290991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"saits_imputed_mnar = impute_and_evaluate(X_mnar, X, window_size=WINDOW, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:01:04.292819Z","iopub.execute_input":"2025-02-21T11:01:04.293109Z","iopub.status.idle":"2025-02-21T11:02:04.692273Z","shell.execute_reply.started":"2025-02-21T11:01:04.293074Z","shell.execute_reply":"2025-02-21T11:02:04.691494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reconstruct to original shape for comparison with \"simple\" methods","metadata":{}},{"cell_type":"code","source":"def reconstruct_time_series_from_windows(\n    windows_3d: np.ndarray, \n    T: int, \n    window_size: int, \n    step: int = 1\n) -> np.ndarray:\n    \"\"\"\n    Reconstructs a 2D array (T, F) from a 3D array (N, window_size, F), \n    assuming the same logic from `create_windows` was used with step=1 (or another step).\n    A simple average is applied at each time position if it appears in multiple windows.\n    \"\"\"\n    N, w_size, F = windows_3d.shape\n    recon = np.zeros((T, F), dtype=np.float32)\n    counts = np.zeros((T, 1), dtype=np.float32)\n    \n    idx = 0\n    for start in range(0, T - window_size + 1, step):\n        end = start + window_size\n        recon[start:end, :] += windows_3d[idx, :, :]  # Sum window values\n        counts[start:end, 0] += 1  # Track the number of contributions\n        idx += 1\n    \n    # Avoid division by zero; counts should be >0 in the valid range\n    nonzero_mask = counts[:, 0] != 0\n    recon[nonzero_mask, :] /= counts[nonzero_mask, 0, np.newaxis]\n    \n    return recon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:04:26.502345Z","iopub.execute_input":"2025-02-21T11:04:26.502664Z","iopub.status.idle":"2025-02-21T11:04:26.507956Z","shell.execute_reply.started":"2025-02-21T11:04:26.502639Z","shell.execute_reply":"2025-02-21T11:04:26.507136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"saits_imputed_mcar_recon = reconstruct_time_series_from_windows(saits_imputed_mcar,\n                                                             X.shape[0], #Original timesteps count\n                                                             window_size=WINDOW, step=1)\nsaits_imputed_mnar_recon = reconstruct_time_series_from_windows(saits_imputed_mnar,\n                                                             X.shape[0], #Original timesteps count\n                                                             window_size=WINDOW, step=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:04:26.663856Z","iopub.execute_input":"2025-02-21T11:04:26.664179Z","iopub.status.idle":"2025-02-21T11:04:26.759557Z","shell.execute_reply.started":"2025-02-21T11:04:26.664154Z","shell.execute_reply":"2025-02-21T11:04:26.758733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate methods","metadata":{}},{"cell_type":"code","source":"# Evaluate methods\nmetrics_mean = executor.evaluate_imputation(\n    imputed_results['mean_mcar'],\n    imputed_results['mean_mnar']\n)\n\nmetrics_interp = executor.evaluate_imputation(\n    imputed_results['interp_mcar'],\n    imputed_results['interp_mnar']\n)\n\nmetrics_knn = executor.evaluate_imputation(\n    imputed_results['knn_mcar'],\n    imputed_results['knn_mnar']\n)\n\nmetrics_saits = executor.evaluate_imputation(\n    saits_imputed_mcar_recon,\n    saits_imputed_mnar_recon\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:06:07.888969Z","iopub.execute_input":"2025-02-21T11:06:07.889303Z","iopub.status.idle":"2025-02-21T11:06:07.901940Z","shell.execute_reply.started":"2025-02-21T11:06:07.889280Z","shell.execute_reply":"2025-02-21T11:06:07.901314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-\" * 60)\nprint(f\"Imputation Metrics for Resampling = {RESAMPLING}, Data Shape = {X.shape}\")\nprint(\"-\" * 60)\nprint(f\"{'Method':<15}{'MCAR_MAE':<12}{'MCAR_RMSE':<12}{'MNAR_MAE':<12}{'MNAR_RMSE':<12}\")\nprint(\"-\" * 60)\n\nmetrics_dict = {\n        \"Mean\": metrics_mean,\n        \"Interpolation\": metrics_interp,\n        \"KNN\": metrics_knn,\n        \"SAITS\": metrics_saits,\n    }\n\nprint(\"-\" * 60)\n\nfor method, metrics in metrics_dict.items():\n    mcar_mae = metrics[\"MCAR\"][\"MAE\"]\n    mcar_rmse = metrics[\"MCAR\"][\"RMSE\"]\n    mnar_mae = metrics[\"MNAR\"][\"MAE\"]\n    mnar_rmse = metrics[\"MNAR\"][\"RMSE\"]\n    print(f\"{method:<15}{mcar_mae:<12.6f}{mcar_rmse:<12.6f}{mnar_mae:<12.6f}{mnar_rmse:<12.6f}\")\n\nprint(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:15:42.039893Z","iopub.execute_input":"2025-02-21T11:15:42.040289Z","iopub.status.idle":"2025-02-21T11:15:42.047824Z","shell.execute_reply.started":"2025-02-21T11:15:42.040241Z","shell.execute_reply":"2025-02-21T11:15:42.047195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_imputation_comparison(\n    metrics_mean: dict, \n    metrics_interp: dict, \n    metrics_knn: dict,\n    metrics_saits: dict,\n    figsize: tuple = (12, 7)\n):\n    \"\"\"\n    Visualize imputation metrics comparison between Mean, Interpolation, KNN, and SAITS for both MCAR and MNAR.\n    Uses a log scale on the y-axis.\n\n    Parameters\n    ----------\n    metrics_mean : dict\n        A dictionary with keys 'MCAR' and 'MNAR' for the Mean imputation metrics.\n    metrics_interp : dict\n        A dictionary with keys 'MCAR' and 'MNAR' for the Interpolation imputation metrics.\n    metrics_knn : dict\n        A dictionary with keys 'MCAR' and 'MNAR' for the KNN imputation metrics.\n    metrics_saits : dict\n        A dictionary with keys 'MCAR' and 'MNAR' for the SAITS imputation metrics.\n    figsize : tuple\n        Figure size (width, height) in inches.\n    \"\"\"\n    plt.figure(figsize=figsize)\n    \n    # Extract metric names from the MCAR dictionary of metrics_mean\n    metric_names = list(metrics_mean['MCAR'].keys())\n    \n    # Prepare values for each category\n    values = {\n        'MCAR (Mean)': list(metrics_mean['MCAR'].values()),\n        'MCAR (Interp)': list(metrics_interp['MCAR'].values()),\n        'MCAR (KNN)': list(metrics_knn['MCAR'].values()),\n        'MCAR (SAITS)': list(metrics_saits['MCAR'].values()),\n        'MNAR (Mean)': list(metrics_mean['MNAR'].values()),\n        'MNAR (Interp)': list(metrics_interp['MNAR'].values()),\n        'MNAR (KNN)': list(metrics_knn['MNAR'].values()),\n        'MNAR (SAITS)': list(metrics_saits['MNAR'].values())\n    }\n    \n    # Define positions for bars\n    x = np.arange(len(metric_names))\n    width = 0.1  # Adjust width to fit all bars properly\n\n    # Use a colormap for better distinction\n    cmap = plt.get_cmap(\"viridis\")\n    colors = [cmap(x_i) for x_i in np.linspace(0.2, 1.0, 8)]\n\n    # Plot bars for each imputation method\n    plt.bar(x - 3.5*width, values['MCAR (Mean)'], width, label='MCAR (Mean)', color=colors[0])\n    plt.bar(x - 2.5*width, values['MCAR (Interp)'], width, label='MCAR (Interp)', color=colors[1])\n    plt.bar(x - 1.5*width, values['MCAR (KNN)'], width, label='MCAR (KNN)', color=colors[2])\n    plt.bar(x - 0.5*width, values['MCAR (SAITS)'], width, label='MCAR (SAITS)', color=colors[3])\n    plt.bar(x + 0.5*width, values['MNAR (Mean)'], width, label='MNAR (Mean)', color=colors[4])\n    plt.bar(x + 1.5*width, values['MNAR (Interp)'], width, label='MNAR (Interp)', color=colors[5])\n    plt.bar(x + 2.5*width, values['MNAR (KNN)'], width, label='MNAR (KNN)', color=colors[6])\n    plt.bar(x + 3.5*width, values['MNAR (SAITS)'], width, label='MNAR (SAITS)', color=colors[7])\n\n    # Customize plot\n    plt.title('Imputation Methods Comparison', pad=20)\n    plt.xlabel('Metrics')\n    plt.xticks(x, metric_names)\n    plt.yscale(\"log\")\n    plt.legend()\n    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:19:08.611112Z","iopub.execute_input":"2025-02-21T11:19:08.611388Z","iopub.status.idle":"2025-02-21T11:19:08.619702Z","shell.execute_reply.started":"2025-02-21T11:19:08.611367Z","shell.execute_reply":"2025-02-21T11:19:08.618949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create visualization\nplot_imputation_comparison(metrics_mean, metrics_interp, metrics_knn, metrics_saits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:19:08.746592Z","iopub.execute_input":"2025-02-21T11:19:08.746848Z","iopub.status.idle":"2025-02-21T11:19:09.140369Z","shell.execute_reply.started":"2025-02-21T11:19:08.746828Z","shell.execute_reply":"2025-02-21T11:19:09.139511Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save results to load on Forecast notebook","metadata":{}},{"cell_type":"code","source":"imputed_results['saits_mcar'] = saits_imputed_mcar_recon\nimputed_results['saits_mnar'] = saits_imputed_mnar_recon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:20:29.755409Z","iopub.execute_input":"2025-02-21T11:20:29.755712Z","iopub.status.idle":"2025-02-21T11:20:29.759118Z","shell.execute_reply.started":"2025-02-21T11:20:29.755687Z","shell.execute_reply":"2025-02-21T11:20:29.758164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_imputed_results(imputed_dict: dict, filename: str = f\"imputed_results_{RESAMPLING}.npz\"):\n    np.savez_compressed(filename, **imputed_dict)\n    print(f\"Saved imputed results to {filename}\")\n\nsave_imputed_results(imputed_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:50.802436Z","iopub.status.idle":"2025-02-21T10:47:50.802840Z","shell.execute_reply":"2025-02-21T10:47:50.802651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_originals(original_data: pl.DataFrame, target: str, save_path: str = \"./originals\"):\n    \"\"\"\n    Saves the train dataset as parquet files and the extracted numpy arrays as .npy files.\n\n    Parameters:\n    - data (pl.DataFrame): original dataset in Polars format.\n    - target (str): Target column name.\n    - save_path (str): Path to save the files.\n    \"\"\"\n    import os\n    os.makedirs(save_path, exist_ok=True)\n    \n    # Convert to numpy\n    y = data.drop(\"Date Time\").select(target).to_numpy()\n    X = data.drop([\"Date Time\", target]).to_numpy()\n\n    # Save Polars DataFrames as Parquet\n    data.write_parquet(f\"{save_path}/train_data.parquet\")\n\n    # Save numpy arrays\n    np.save(f\"{save_path}/X.npy\", X)\n    np.save(f\"{save_path}/y.npy\", y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:50.803629Z","iopub.status.idle":"2025-02-21T10:47:50.803936Z","shell.execute_reply":"2025-02-21T10:47:50.803816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_originals(data, target=TARGET)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:47:50.804792Z","iopub.status.idle":"2025-02-21T10:47:50.805221Z","shell.execute_reply":"2025-02-21T10:47:50.805025Z"}},"outputs":[],"execution_count":null}]}